{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xP00WlaMWBZC"},"source":["## Skip-gram and Negative Sampling "]},{"cell_type":"markdown","metadata":{"id":"mk4-Hpe1CH16"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"RutaI-Tpev3T","executionInfo":{"status":"ok","timestamp":1669685749002,"user_tz":-540,"elapsed":9744,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["import io\n","import re\n","import string\n","import tensorflow as tf\n","import tqdm\n","\n","from tensorflow.keras import Model\n","from tensorflow.keras.layers import Dot, Embedding, Flatten\n","from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"XkJ5299Tek6B","executionInfo":{"status":"ok","timestamp":1669685749003,"user_tz":-540,"elapsed":17,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["SEED = 42\n","AUTOTUNE = tf.data.AUTOTUNE"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RW-g5buCHwh3"},"source":["### Vectorize an example sentence"]},{"cell_type":"markdown","metadata":{"id":"y8TfZIgoQrcP"},"source":["Consider the following sentence:    \n","`The wide road shimmered in the hot sun.`\n","\n","Tokenize the sentence:"]},{"cell_type":"code","metadata":{"id":"bsl7jBzV6_KK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3c2f4445-96bc-4d73-e1ed-90eea6fe913e","executionInfo":{"status":"ok","timestamp":1669685749003,"user_tz":-540,"elapsed":16,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["sentence = \"The wide road shimmered in the hot sun\"\n","tokens = list(sentence.lower().split())\n","print(tokens)\n","print(len(tokens))"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["['the', 'wide', 'road', 'shimmered', 'in', 'the', 'hot', 'sun']\n","8\n"]}]},{"cell_type":"markdown","metadata":{"id":"PU-bs1XtThEw"},"source":["Create a vocabulary to save mappings from tokens to integer indices."]},{"cell_type":"code","metadata":{"id":"UdYv1HJUQ8XA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c78acfcb-6ec5-4fc5-e5e5-91b7ecec29dc","executionInfo":{"status":"ok","timestamp":1669685749003,"user_tz":-540,"elapsed":15,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["vocab, index = {}, 1  # start indexing from 1\n","vocab['<pad>'] = 0  # add a padding token\n","for token in tokens:\n","  if token not in vocab:\n","    vocab[token] = index\n","    index += 1\n","vocab_size = len(vocab)\n","print(vocab)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZpuP43Dddasr"},"source":["Create an inverse vocabulary to save mappings from integer indices to tokens."]},{"cell_type":"code","metadata":{"id":"o9ULAJYtEvKl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a2dc0125-a113-4aab-b470-39e2bb849f38","executionInfo":{"status":"ok","timestamp":1669685749003,"user_tz":-540,"elapsed":13,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["inverse_vocab = {index: token for token, index in vocab.items()}\n","print(inverse_vocab)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n"]}]},{"cell_type":"markdown","metadata":{"id":"n3qtuyxIRyii"},"source":["Vectorize your sentence.\n"]},{"cell_type":"code","metadata":{"id":"CsB3-9uQQYyl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d9677585-ae97-4a98-9f8c-29f70e6f7970","executionInfo":{"status":"ok","timestamp":1669685749004,"user_tz":-540,"elapsed":13,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["example_sequence = [vocab[word] for word in tokens]\n","print(example_sequence)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 2, 3, 4, 5, 1, 6, 7]\n"]}]},{"cell_type":"markdown","metadata":{"id":"ox1I28JRIOdM"},"source":["### Generate skip-grams from one sentence"]},{"cell_type":"markdown","metadata":{"id":"t7NNKAmSiHvy"},"source":["The `tf.keras.preprocessing.sequence` module provides useful functions that simplify data preparation for Word2Vec. You can use the `tf.keras.preprocessing.sequence.skipgrams` to generate skip-gram pairs from the `example_sequence` with a given `window_size` from tokens in the range `[0, vocab_size)`.\n","\n","Note: `negative_samples` is set to `0` here as batching negative samples generated by this function requires a bit of code. You will use another function to perform negative sampling in the next section.\n"]},{"cell_type":"code","metadata":{"id":"USAJxW4RD7pn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4661f90b-b506-4e38-ca10-eb3c7a504318","executionInfo":{"status":"ok","timestamp":1669685749004,"user_tz":-540,"elapsed":13,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["window_size = 2\n","positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n","      example_sequence,\n","      vocabulary_size=vocab_size,\n","      window_size=window_size,\n","      negative_samples=0, shuffle=True) ## shuffle=False     # negative 뽑지 않음 = 0\n","\n","\n","print(positive_skip_grams)\n","print(len(positive_skip_grams))"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[[4, 5], [6, 5], [3, 5], [2, 3], [5, 1], [6, 7], [1, 2], [1, 4], [5, 6], [1, 6], [6, 1], [4, 3], [3, 2], [3, 1], [1, 7], [1, 3], [7, 1], [2, 4], [7, 6], [1, 5], [4, 1], [4, 2], [5, 3], [3, 4], [2, 1], [5, 4]]\n","26\n"]}]},{"cell_type":"markdown","metadata":{"id":"uc9uhiMwY-AQ"},"source":["Take a look at few positive skip-grams."]},{"cell_type":"code","metadata":{"id":"SCnqEukIE9pt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6ccb305f-a453-4285-b9e3-7d59128c1873","executionInfo":{"status":"ok","timestamp":1669685749004,"user_tz":-540,"elapsed":12,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["for target, context in positive_skip_grams[:5]:\n","  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["(4, 5): (shimmered, in)\n","(6, 5): (hot, in)\n","(3, 5): (road, in)\n","(2, 3): (wide, road)\n","(5, 1): (in, the)\n"]}]},{"cell_type":"markdown","metadata":{"id":"_ua9PkMTISF0"},"source":["### Negative sampling for one skip-gram "]},{"cell_type":"markdown","metadata":{"id":"Esqn8WBfZnEK"},"source":["The `skipgrams` function returns all positive skip-gram pairs by sliding over a given window span. To produce additional skip-gram pairs that would serve as negative samples for training, you need to sample random words from the vocabulary. Use the `tf.random.log_uniform_candidate_sampler` function to sample `num_ns` number of negative samples for a given target word in a window. You can call the function on one skip-grams's target word and pass the context word as true class to exclude it from being sampled.\n"]},{"cell_type":"markdown","metadata":{"id":"AgH3aSvw3xTD"},"source":["Key point: *num_ns* (number of negative samples per positive context word) between [5, 20] is [shown to work](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) best for smaller datasets, while *num_ns* between [2,5] suffices for larger datasets. "]},{"cell_type":"code","metadata":{"id":"m_LmdzqIGr5L","colab":{"base_uri":"https://localhost:8080/"},"outputId":"499d3cce-36f9-4bea-91c8-8cb5bf64ea69","executionInfo":{"status":"ok","timestamp":1669685942737,"user_tz":-540,"elapsed":427,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["# Get target and context words for one positive skip-gram.\n","target_word, context_word = positive_skip_grams[0]\n","\n","# Set the number of negative samples per positive context.\n","num_ns = 4\n","\n","context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n","negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n","    true_classes=context_class,  # class that should be sampled as 'positive'       # context_class 이거 뺴고 뽑아 \n","    num_true=1,  # each positive skip-gram has 1 positive context class\n","    num_sampled=num_ns,  # number of negative context words to sample\n","    unique=True,  # all the negative samples should be unique                      # unique=True 은 비복원추출\n","    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n","    seed=SEED,  # seed for reproducibility\n","    name=\"negative_sampling\"  # name of this operation\n",")\n","print(negative_sampling_candidates)\n","print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])\n","\n","\n","\n","\n","# context_class 이거 뺴고 뽑은 것 중 num_sampled=num_ns 개수만큼 랜덤하게 뽑은 "],"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([86 30 36 42], shape=(4,), dtype=int64)\n","['must', 'thy', 'we', 'do']\n"]}]},{"cell_type":"markdown","metadata":{"id":"8MSxWCrLIalp"},"source":["### Construct one training example"]},{"cell_type":"markdown","metadata":{"id":"Q6uEWdj8vKKv"},"source":["For a given positive `(target_word, context_word)` skip-gram, you now also have `num_ns` negative sampled context words that do not appear in the window size neighborhood of `target_word`. Batch the `1` positive `context_word` and `num_ns` negative context words into one tensor. This produces a set of positive skip-grams (labelled as `1`) and negative samples (labelled as `0`) for each target word."]},{"cell_type":"code","metadata":{"id":"zSiZwifuLvHf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669685944949,"user_tz":-540,"elapsed":399,"user":{"displayName":"정다인","userId":"05942020947635196780"}},"outputId":"9c1684ee-ffa5-4c99-b3f3-4949585fa599"},"source":["# Add a dimension so you can use concatenation (on the next step).\n","negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)     #negative_sampling_candidates 차원짜리 벡터 EX) 4 ->  4 * 1짜리 벡터로\n","\n","# Concat positive context word with negative sampled words.\n","context = tf.concat([context_class, negative_sampling_candidates], 0)\n","\n","# Label first context word as 1 (positive) followed by num_ns 0s (negative).\n","label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n","\n","# Reshape target to shape (1,) and context and label to (num_ns+1,).\n","target = tf.squeeze(target_word)\n","context = tf.squeeze(context)\n","label = tf.squeeze(label)\n","\n","print(target)\n","print(context)\n","print(label)"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(4, shape=(), dtype=int32)\n","tf.Tensor([ 5 86 30 36 42], shape=(5,), dtype=int64)\n","tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n"]}]},{"cell_type":"markdown","metadata":{"id":"OIJeoFCAwtXJ"},"source":["Take a look at the context and the corresponding labels for the target word from the skip-gram example above. "]},{"cell_type":"code","metadata":{"id":"tzyCPCuZwmdL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9827f7e2-179e-445a-8c18-a53b4439480b","executionInfo":{"status":"ok","timestamp":1669686124727,"user_tz":-540,"elapsed":334,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["print(f\"target_index    : {target}\")\n","print(f\"target_word     : {inverse_vocab[target_word]}\")\n","print(f\"context_indices : {context}\")\n","print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n","print(f\"label           : {label}\")"],"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["target_index    : 4\n","target_word     : to\n","context_indices : [ 5 86 30 36 42]\n","context_words   : ['i', 'must', 'thy', 'we', 'do']\n","label           : [1 0 0 0 0]\n"]}]},{"cell_type":"markdown","metadata":{"id":"9wmdO_MEIpaM"},"source":["## Compile all steps into one function\n"]},{"cell_type":"markdown","metadata":{"id":"iLKwNAczHsKg"},"source":["### Skip-gram Sampling table "]},{"cell_type":"code","metadata":{"id":"Rn9zAnDccyRg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"abc9e9db-58a6-41ec-8d82-ddebe8e95da2","executionInfo":{"status":"ok","timestamp":1669685749460,"user_tz":-540,"elapsed":4,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\n","print(sampling_table)\n","print((sampling_table).sum())\n"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n"," 0.01212381 0.01347162 0.01474487 0.0159558 ]\n","0.09530464658339152\n"]}]},{"cell_type":"markdown","metadata":{"id":"EHvSptcPk5fp"},"source":["`sampling_table[i]` denotes the probability of sampling the i-th most common word in a dataset. The function assumes a [Zipf's distribution](https://en.wikipedia.org/wiki/Zipf%27s_law) of the word frequencies for sampling."]},{"cell_type":"markdown","metadata":{"id":"mRHMssMmHgH-"},"source":["Key point: The `tf.random.log_uniform_candidate_sampler` already assumes that the vocabulary frequency follows a log-uniform (Zipf's) distribution. Using these distribution weighted sampling also helps approximate the Noise Contrastive Estimation (NCE) loss with simpler loss functions for training a negative sampling objective."]},{"cell_type":"markdown","metadata":{"id":"aj--8RFK6fgW"},"source":["### Generate training data"]},{"cell_type":"markdown","metadata":{"id":"dy5hl4lQ0B2M"},"source":["Compile all the steps described above into a function that can be called on a list of vectorized sentences obtained from any text dataset. Notice that the sampling table is built before sampling skip-gram word pairs. You will use this function in the later sections."]},{"cell_type":"code","metadata":{"id":"63INISDEX1Hu","executionInfo":{"status":"ok","timestamp":1669685749461,"user_tz":-540,"elapsed":5,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["# Generates skip-gram pairs with negative sampling for a list of sequences\n","# (int-encoded sentences) based on window size, number of negative samples\n","# and vocabulary size.\n","def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n","  # Elements of each training example are appended to these lists.\n","  targets, contexts, labels = [], [], []\n","\n","  # Build the sampling table for vocab_size tokens.\n","  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n","\n","  # Iterate over all sequences (sentences) in dataset.\n","  for sequence in tqdm.tqdm(sequences):\n","\n","    # Generate positive skip-gram pairs for a sequence (sentence).\n","    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n","          sequence,\n","          vocabulary_size=vocab_size,\n","          sampling_table=sampling_table,\n","          window_size=window_size,\n","          negative_samples=0)\n","\n","    # Iterate over each positive skip-gram pair to produce training examples\n","    # with positive context word and negative samples.\n","    for target_word, context_word in positive_skip_grams:\n","      context_class = tf.expand_dims(\n","          tf.constant([context_word], dtype=\"int64\"), 1)\n","      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n","          true_classes=context_class,\n","          num_true=1,\n","          num_sampled=num_ns,\n","          unique=True,\n","          range_max=vocab_size,\n","          seed=SEED,\n","          name=\"negative_sampling\")\n","\n","\n","      # Build context and label vectors (for one target word)\n","      negative_sampling_candidates = tf.expand_dims(\n","          negative_sampling_candidates, 1)\n","\n","      context = tf.concat([context_class, negative_sampling_candidates], 0)\n","      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n","\n","      # Append each element from the training example to global lists.\n","      targets.append(target_word)\n","      contexts.append(context)\n","      labels.append(label)\n","\n","  return targets, contexts, labels"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"shvPC8Ji2cMK"},"source":["## Prepare training data for Word2Vec"]},{"cell_type":"markdown","metadata":{"id":"j5mbZsZu6uKg"},"source":["With an understanding of how to work with one sentence for a skip-gram negative sampling based Word2Vec model, you can proceed to generate training examples from a larger list of sentences!"]},{"cell_type":"markdown","metadata":{"id":"OFlikI6L26nh"},"source":["### Download text corpus\n"]},{"cell_type":"markdown","metadata":{"id":"rEFavOgN98al"},"source":["You will use a text file of Shakespeare's writing for this tutorial. Change the following line to run this code on your own data."]},{"cell_type":"code","metadata":{"id":"QFkitxzVVaAi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669685749762,"user_tz":-540,"elapsed":305,"user":{"displayName":"정다인","userId":"05942020947635196780"}},"outputId":"d653d89e-af43-43bf-d5ba-7144b3c6cf0f"},"source":["path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n","1115394/1115394 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"markdown","metadata":{"id":"sOsbLq8a37dr"},"source":["Read text from the file and take a look at the first few lines. "]},{"cell_type":"code","metadata":{"id":"lfgnsUw3ofMD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6d9040b4-e8b1-4b56-f511-727e3ebc76a8","executionInfo":{"status":"ok","timestamp":1669685749762,"user_tz":-540,"elapsed":6,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["with open(path_to_file) as f: \n","  lines = f.read().splitlines()\n","for line in lines[:20]:\n","  print(line)"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you know Caius Marcius is chief enemy to the people.\n","\n","All:\n","We know't, we know't.\n","\n","First Citizen:\n","Let us kill him, and we'll have corn at our own price.\n"]}]},{"cell_type":"markdown","metadata":{"id":"gTNZYqUs5C2V"},"source":["Use the non empty lines to construct a `tf.data.TextLineDataset` object for next steps."]},{"cell_type":"code","metadata":{"id":"ViDrwy-HjAs9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b9afa4c5-b4f9-45ee-fd80-5ed8a4fef6d6","executionInfo":{"status":"ok","timestamp":1669685750071,"user_tz":-540,"elapsed":313,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))   # cast \" 변수 종류 바꾸기 -> BOOL\" True만 뽑아내라. \n","\n","i=1\n","for element in text_ds.as_numpy_iterator():\n","  print(element)\n","  i+=1\n","  if i==5: break\n"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["b'First Citizen:'\n","b'Before we proceed any further, hear me speak.'\n","b'All:'\n","b'Speak, speak.'\n"]}]},{"cell_type":"markdown","source":["- b 는 string 변수 형태 표시하기 위해 : 텐서플로우 "],"metadata":{"id":"4bt71SAwF8Im"}},{"cell_type":"markdown","metadata":{"id":"vfsc88zE9upk"},"source":["### Vectorize sentences from the corpus"]},{"cell_type":"markdown","metadata":{"id":"XfgZo8zR94KK"},"source":["You can use the `TextVectorization` layer to vectorize sentences from the corpus. Learn more about using this layer in this [Text Classification](https://www.tensorflow.org/tutorials/keras/text_classification) tutorial. Notice from the first few sentences above that the text needs to be in one case and punctuation needs to be removed. To do this, define a `custom_standardization function` that can be used in the TextVectorization layer."]},{"cell_type":"code","metadata":{"id":"2MlsXzo-ZlfK","executionInfo":{"status":"ok","timestamp":1669685750772,"user_tz":-540,"elapsed":705,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["# Now, create a custom standardization function to lowercase the text and\n","# remove punctuation.\n","def custom_standardization(input_data):\n","  lowercase = tf.strings.lower(input_data)\n","  return tf.strings.regex_replace(lowercase,                                           # lowercase를\n","                                  '[%s]' % re.escape(string.punctuation), '')\n","\n","\n","# Define the vocabulary size and number of words in a sequence.\n","vocab_size = 4096\n","sequence_length = 10\n","\n","# Use the text vectorization layer to normalize, split, and map strings to\n","# integers. Set output_sequence_length length to pad all samples to same length.\n","vectorize_layer = TextVectorization(\n","    standardize=custom_standardization,\n","    max_tokens=vocab_size,\n","    output_mode='int',\n","    output_sequence_length=sequence_length)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g92LuvnyBmz1"},"source":["Call `adapt` on the text dataset to create vocabulary.\n"]},{"cell_type":"code","metadata":{"id":"seZau_iYMPFT","executionInfo":{"status":"ok","timestamp":1669685753121,"user_tz":-540,"elapsed":2354,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["vectorize_layer.adapt(text_ds.batch(1024)) ## creating vocab set  1024씩 처리하셈. 너무 기니까"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jg2z7eeHMnH-"},"source":["Once the state of the layer has been adapted to represent the text corpus, the vocabulary can be accessed with `get_vocabulary()`. This function returns a list of all vocabulary tokens sorted (descending) by their frequency. "]},{"cell_type":"code","metadata":{"id":"jgw9pTA7MRaU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0d837c6b-ba11-4a7c-dc4c-3301d32a3df9","executionInfo":{"status":"ok","timestamp":1669685753121,"user_tz":-540,"elapsed":3,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["# Save the created vocabulary for reference.\n","inverse_vocab = vectorize_layer.get_vocabulary()\n","print(len(inverse_vocab))\n","print(inverse_vocab[:20])   # 빈도수 기준 상위 20"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["4096\n","['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a', 'that', 'in', 'is', 'not', 'for', 'with', 'me', 'it', 'be', 'your']\n"]}]},{"cell_type":"markdown","metadata":{"id":"DOQ30Tx6KA2G"},"source":["The vectorize_layer can now be used to generate vectors for each element in the `text_ds`."]},{"cell_type":"code","metadata":{"id":"yUVYrDp0araQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d93396db-3308-4095-ae03-21255f0d2cb1","executionInfo":{"status":"ok","timestamp":1669685753122,"user_tz":-540,"elapsed":4,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["# Vectorize the data in text_ds.\n","## prefetch: 전처리와 훈련 스텝 모델 실행 오버랩 => 계산 시간 단축하게끔 하는 목적 (map실행하는 동안 그 다음 데이터 불러옴)\n","## map(fun): data에 fun 함수 실행\n","## unbatch => batch를 없앰\n","text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch() \n","print(text_vector_ds)"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["<_UnbatchDataset element_spec=TensorSpec(shape=(10,), dtype=tf.int64, name=None)>\n"]}]},{"cell_type":"markdown","metadata":{"id":"7YyH_SYzB72p"},"source":["### Obtain sequences from the dataset"]},{"cell_type":"markdown","metadata":{"id":"NFUQLX0_KaRC"},"source":["You now have a `tf.data.Dataset` of integer encoded sentences. To prepare the dataset for training a Word2Vec model, flatten the dataset into a list of sentence vector sequences. This step is required as you would iterate over each sentence in the dataset to produce positive and negative examples. \n","\n","Note: Since the `generate_training_data()` defined earlier uses non-TF python/numpy functions, you could also use a `tf.py_function` or `tf.numpy_function` with `tf.data.Dataset.map()`."]},{"cell_type":"code","metadata":{"id":"sGXoOh9y11pM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5742d62a-0bb9-4f13-8230-aacb72c6ea66","executionInfo":{"status":"ok","timestamp":1669685758442,"user_tz":-540,"elapsed":5323,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["sequences = list(text_vector_ds.as_numpy_iterator())\n","print(len(sequences))"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["32777\n"]}]},{"cell_type":"markdown","metadata":{"id":"tDc4riukLTqg"},"source":["Take a look at few examples from `sequences`.\n"]},{"cell_type":"code","metadata":{"id":"WZf1RIbB2Dfb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8c50941e-e673-4dde-eb59-6914d32732ca","executionInfo":{"status":"ok","timestamp":1669685758443,"user_tz":-540,"elapsed":4,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["for seq in sequences[:5]:\n","  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n","[138  36 982 144 673 125  16 106   0   0] => ['before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '', '']\n","[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n","[106 106   0   0   0   0   0   0   0   0] => ['speak', 'speak', '', '', '', '', '', '', '', '']\n","[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n"]}]},{"cell_type":"markdown","metadata":{"id":"yDzSOjNwCWNh"},"source":["### Generate training examples from sequences"]},{"cell_type":"markdown","metadata":{"id":"BehvYr-nEKyY"},"source":["`sequences` is now a list of int encoded sentences. Just call the `generate_training_data()` function defined earlier to generate training examples for the Word2Vec model. To recap, the function iterates over each word from each sequence to collect positive and negative context words. Length of target, contexts and labels should be same, representing the total number of training examples."]},{"cell_type":"code","metadata":{"id":"44DJ22M6nX5o","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0cb64682-7472-4dee-ac6e-28a31c15dd86","executionInfo":{"status":"ok","timestamp":1669685774856,"user_tz":-540,"elapsed":16416,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["targets, contexts, labels = generate_training_data(\n","    sequences=sequences,\n","    window_size=2,\n","    num_ns=4,\n","    vocab_size=vocab_size,\n","    seed=SEED)\n","\n","print(len(targets), len(contexts), len(labels))"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 32777/32777 [00:16<00:00, 2015.48it/s]"]},{"output_type":"stream","name":"stdout","text":["64777 64777 64777\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","metadata":{"id":"97PqsusOFEpc"},"source":["### Configure the dataset for performance"]},{"cell_type":"markdown","metadata":{"id":"7jnFVySViQTj"},"source":["To perform efficient batching for the potentially large number of training examples, use the `tf.data.Dataset` API. After this step, you would have a `tf.data.Dataset` object of `(target_word, context_word), (label)` elements to train your Word2Vec model!"]},{"cell_type":"code","metadata":{"id":"nbu8PxPSnVY2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"04a4dfb6-9bd9-4434-973c-749d70804777","executionInfo":{"status":"ok","timestamp":1669685779548,"user_tz":-540,"elapsed":4696,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["BATCH_SIZE = 1024 ## batch size\n","BUFFER_SIZE = 10000  ## shuffle하는데 필요한 수치.\n","## slice되어 있는 데이터를 합쳐서 train data 만듬\n","## input: (targets (1),contexts (1+4)), output: labels (5)\n","dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels)) \n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","print(dataset)"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["<BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int32, name=None), TensorSpec(shape=(1024, 5, 1), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"]}]},{"cell_type":"markdown","metadata":{"id":"tyrNX6Fs6K3F"},"source":["Add `cache()` and `prefetch()` to improve performance."]},{"cell_type":"code","metadata":{"id":"Y5Ueg6bcFPVL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0a07586f-23a9-40a1-c4de-d849bdd8d143","executionInfo":{"status":"ok","timestamp":1669685779548,"user_tz":-540,"elapsed":15,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["## cache, prefetch 모두 효율적인 계산시간을 위해 사용하는 것.\n","dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n","print(dataset)"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["<PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int32, name=None), TensorSpec(shape=(1024, 5, 1), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"]}]},{"cell_type":"markdown","metadata":{"id":"1S-CmUMszyEf"},"source":["## Model and Training"]},{"cell_type":"markdown","metadata":{"id":"sQFqaBMPwBqC"},"source":["The Word2Vec model can be implemented as a classifier to distinguish between true context words from skip-grams and false context words obtained through negative sampling. You can perform a dot product between the embeddings of target and context words to obtain predictions for labels and compute loss against true labels in the dataset."]},{"cell_type":"markdown","metadata":{"id":"oc7kTbiwD9sy"},"source":["### Subclassed Word2Vec Model"]},{"cell_type":"markdown","metadata":{"id":"Jvr9pM1G1sQN"},"source":["Use the [Keras Subclassing API](https://www.tensorflow.org/guide/keras/custom_layers_and_models) to define your Word2Vec model with the following layers:\n","\n","\n","* `target_embedding`: A `tf.keras.layers.Embedding` layer which looks up the embedding of a word when it appears as a target word. The number of parameters in this layer are `(vocab_size * embedding_dim)`.\n","* `context_embedding`: Another `tf.keras.layers.Embedding` layer which looks up the embedding of a word when it appears as a context word. The number of parameters in this layer are the same as those in `target_embedding`, i.e. `(vocab_size * embedding_dim)`.\n","* `dots`: A `tf.keras.layers.Dot` layer that computes the dot product of target and context embeddings from a training pair.\n","* `flatten`: A `tf.keras.layers.Flatten` layer to flatten the results of `dots` layer into logits.\n","\n","With the subclassed model, you can define the `call()` function that accepts `(target, context)` pairs which can then be passed into their corresponding embedding layer. Reshape the `context_embedding` to perform a dot product with `target_embedding` and return the flattened result."]},{"cell_type":"markdown","metadata":{"id":"KiAwuIqqw7-7"},"source":["Key point: The `target_embedding` and `context_embedding` layers can be shared as well. You could also use a concatenation of both embeddings as the final Word2Vec embedding."]},{"cell_type":"code","metadata":{"id":"i9ec-sS6xd8Z","executionInfo":{"status":"ok","timestamp":1669685779548,"user_tz":-540,"elapsed":14,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["class Word2Vec(Model):\n","  def __init__(self, vocab_size, embedding_dim):\n","    super(Word2Vec, self).__init__()\n","    ## Embedding: integer to dense vector\n","    ## target_embedding: target을 embedding하기 위해 필요한 layer\n","    self.target_embedding = Embedding(vocab_size,\n","                                      embedding_dim,\n","                                      input_length=1,\n","                                      name=\"w2v_embedding\")\n","    ## context_embedding: context를 embedding하기 위해 필요한 layerR\n","    self.context_embedding = Embedding(vocab_size,\n","                                       embedding_dim,\n","                                       input_length=num_ns+1) ## context는 5개이므로\n","    self.dots = Dot(axes=(3, 1))  ## 두 개의 embedding vector를 내적\n","    self.flatten = Flatten()  ## 내적 결과를 일렬로 나열\n","\n","  def call(self, pair):\n","    target, context = pair\n","    word_emb = self.target_embedding(target)\n","    context_emb = self.context_embedding(context)\n","    dots = self.dots([context_emb, word_emb])\n","    return self.flatten(dots)"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["  ## target : 1024 -> word_emb : 1024, 128\n","  ## context : 1024, 5, 1 ->   1024, 5, 1, 128\n","  ## dots : 1024, 5, 1\n","  ## flatten : 1024, 5  (label 과 같은 차원을 맞춰주기 위함) ex : 1,0,0,0,0"],"metadata":{"id":"8sZo98wrKG1_"}},{"cell_type":"markdown","metadata":{"id":"-RLKz9LFECXu"},"source":["### Define loss function and compile model\n"]},{"cell_type":"code","metadata":{"id":"ekQg_KbWnnmQ","executionInfo":{"status":"ok","timestamp":1669685779548,"user_tz":-540,"elapsed":14,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["## 모형 만들고 loss function, optimizer 정해주기\n","embedding_dim = 128\n","word2vec = Word2Vec(vocab_size, embedding_dim)\n","word2vec.compile(optimizer='adam',\n","                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n","                 metrics=['accuracy'])"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P3MUMrluqNX2"},"source":["Also define a callback to log training statistics for tensorboard."]},{"cell_type":"markdown","metadata":{"id":"h5wEBotlGZ7B"},"source":["Train the model with `dataset` prepared above for some number of epochs."]},{"cell_type":"code","metadata":{"id":"gmC1BJalEZIY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e5fd6082-66d4-4557-f522-e4150d180294","executionInfo":{"status":"ok","timestamp":1669685859731,"user_tz":-540,"elapsed":80196,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["## 학습 시작\n","word2vec.fit(dataset, epochs=30)"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","63/63 [==============================] - 3s 30ms/step - loss: 1.6083 - accuracy: 0.2337\n","Epoch 2/30\n","63/63 [==============================] - 2s 27ms/step - loss: 1.5891 - accuracy: 0.5467\n","Epoch 3/30\n","63/63 [==============================] - 2s 27ms/step - loss: 1.5417 - accuracy: 0.5879\n","Epoch 4/30\n","63/63 [==============================] - 2s 29ms/step - loss: 1.4604 - accuracy: 0.5632\n","Epoch 5/30\n","63/63 [==============================] - 2s 26ms/step - loss: 1.3638 - accuracy: 0.5746\n","Epoch 6/30\n","63/63 [==============================] - 2s 27ms/step - loss: 1.2670 - accuracy: 0.6055\n","Epoch 7/30\n","63/63 [==============================] - 2s 26ms/step - loss: 1.1757 - accuracy: 0.6414\n","Epoch 8/30\n","63/63 [==============================] - 2s 24ms/step - loss: 1.0907 - accuracy: 0.6776\n","Epoch 9/30\n","63/63 [==============================] - 2s 27ms/step - loss: 1.0118 - accuracy: 0.7104\n","Epoch 10/30\n","63/63 [==============================] - 3s 48ms/step - loss: 0.9388 - accuracy: 0.7398\n","Epoch 11/30\n","63/63 [==============================] - 3s 49ms/step - loss: 0.8712 - accuracy: 0.7642\n","Epoch 12/30\n","63/63 [==============================] - 3s 48ms/step - loss: 0.8090 - accuracy: 0.7865\n","Epoch 13/30\n","63/63 [==============================] - 3s 52ms/step - loss: 0.7518 - accuracy: 0.8058\n","Epoch 14/30\n","63/63 [==============================] - 3s 44ms/step - loss: 0.6994 - accuracy: 0.8219\n","Epoch 15/30\n","63/63 [==============================] - 3s 44ms/step - loss: 0.6515 - accuracy: 0.8374\n","Epoch 16/30\n","63/63 [==============================] - 2s 32ms/step - loss: 0.6078 - accuracy: 0.8508\n","Epoch 17/30\n","63/63 [==============================] - 3s 40ms/step - loss: 0.5680 - accuracy: 0.8635\n","Epoch 18/30\n","63/63 [==============================] - 2s 38ms/step - loss: 0.5317 - accuracy: 0.8746\n","Epoch 19/30\n","63/63 [==============================] - 2s 37ms/step - loss: 0.4988 - accuracy: 0.8846\n","Epoch 20/30\n","63/63 [==============================] - 2s 39ms/step - loss: 0.4688 - accuracy: 0.8931\n","Epoch 21/30\n","63/63 [==============================] - 2s 35ms/step - loss: 0.4415 - accuracy: 0.9010\n","Epoch 22/30\n","63/63 [==============================] - 2s 35ms/step - loss: 0.4166 - accuracy: 0.9086\n","Epoch 23/30\n","63/63 [==============================] - 2s 27ms/step - loss: 0.3940 - accuracy: 0.9149\n","Epoch 24/30\n","63/63 [==============================] - 2s 26ms/step - loss: 0.3733 - accuracy: 0.9203\n","Epoch 25/30\n","63/63 [==============================] - 3s 44ms/step - loss: 0.3543 - accuracy: 0.9250\n","Epoch 26/30\n","63/63 [==============================] - 2s 27ms/step - loss: 0.3370 - accuracy: 0.9295\n","Epoch 27/30\n","63/63 [==============================] - 2s 27ms/step - loss: 0.3211 - accuracy: 0.9333\n","Epoch 28/30\n","63/63 [==============================] - 1s 19ms/step - loss: 0.3065 - accuracy: 0.9367\n","Epoch 29/30\n","63/63 [==============================] - 2s 27ms/step - loss: 0.2931 - accuracy: 0.9395\n","Epoch 30/30\n","63/63 [==============================] - 2s 27ms/step - loss: 0.2807 - accuracy: 0.9419\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f0b40160810>"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"TaDW2tIIz8fL"},"source":["## Embedding lookup and analysis"]},{"cell_type":"markdown","metadata":{"id":"Zp5rv01WG2YA"},"source":["Obtain the weights from the model using `get_layer()` and `get_weights()`. The `get_vocabulary()` function provides the vocabulary to build a metadata file with one token per line. "]},{"cell_type":"code","source":["word2vec.get_layer('w2v_embedding').get_weights()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6v0UCdL8Lwhf","executionInfo":{"status":"ok","timestamp":1669688398195,"user_tz":-540,"elapsed":290,"user":{"displayName":"정다인","userId":"05942020947635196780"}},"outputId":"bafc2dfe-1257-40e4-8417-5786a4f6b059"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([[-0.02513342,  0.03719174, -0.01053281, ...,  0.03800357,\n","          0.0245422 , -0.03417538],\n","        [-0.1177707 ,  0.17044562, -0.45756817, ..., -0.53201795,\n","         -0.02582205,  0.17690413],\n","        [-0.09348251,  0.15607134, -0.6326013 , ...,  0.06154591,\n","         -0.478574  ,  0.08689316],\n","        ...,\n","        [-0.16618694,  0.2946829 ,  0.06358801, ..., -0.07948469,\n","          0.09585428,  0.21353276],\n","        [ 0.12524194, -0.18285824,  0.30420834, ..., -0.04559448,\n","          0.01384903,  0.198292  ],\n","        [ 0.03209055,  0.04292702,  0.09761377, ...,  0.19488823,\n","         -0.15908404,  0.01632194]], dtype=float32)]"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"_Uamp1YH8RzU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a21c764e-777a-4e52-a92d-601877583fe6","executionInfo":{"status":"ok","timestamp":1669688410801,"user_tz":-540,"elapsed":270,"user":{"displayName":"정다인","userId":"05942020947635196780"}}},"source":["## embedding vector 살펴보기\n","weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n","vocab = vectorize_layer.get_vocabulary()\n","\n","print(vocab[2])\n","print(weights[2])"],"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["the\n","[-9.34825093e-02  1.56071335e-01 -6.32601321e-01  3.39980006e-01\n","  2.87253737e-01  1.89381614e-01 -5.52521288e-01  1.38086498e-01\n","  2.59725213e-01  1.16115145e-01  4.35260445e-01 -6.46997169e-02\n","  4.03349757e-01 -4.66348737e-01  5.06261468e-01 -2.10431874e-01\n","  2.88908511e-01  3.80315371e-02 -1.76113024e-01  1.89207762e-01\n","  7.86827877e-02  1.32178798e-01 -1.63359448e-01  2.75066979e-02\n","  9.54768360e-02 -2.41752863e-01 -2.36765802e-01 -1.66875854e-01\n","  2.35400215e-01  4.95101184e-01  3.64230841e-01  2.62189180e-01\n","  3.49348456e-01  1.25922695e-01 -1.04805298e-01 -9.91582945e-02\n","  2.16945872e-01 -1.75027430e-01 -6.66567162e-02 -1.27007617e-02\n","  4.43117283e-02 -1.78878501e-01  2.13328272e-01  2.45017916e-01\n"," -2.63587683e-01 -2.08952576e-01 -4.22608793e-01  1.21816173e-01\n","  1.05944306e-01 -2.37400040e-01  1.41065598e-01  1.23708472e-01\n"," -2.96245575e-01  9.95864198e-02  2.94463933e-01  1.22015357e-01\n"," -4.57480490e-01  9.56135914e-02 -1.99018434e-01  1.77657291e-01\n","  1.88235957e-02 -1.12260869e-02 -3.16276282e-01 -2.74012864e-01\n"," -4.14659530e-01  5.90196066e-02 -8.65956470e-02 -1.86620474e-01\n"," -1.13985360e-01 -4.95920688e-01 -9.11097229e-02 -2.17414141e-01\n","  4.86985147e-01  2.95899451e-01  3.16052049e-01 -2.00774759e-01\n"," -1.05437428e-01  2.02631980e-01 -3.56515869e-02  3.00717562e-01\n"," -1.62313183e-04 -4.88984168e-01 -1.06501304e-01  2.08901212e-01\n"," -3.76388192e-01  1.83770612e-01 -1.94535598e-01 -1.81849301e-01\n"," -6.81889474e-01 -1.03332892e-01  1.97697490e-01  2.34938618e-02\n","  1.88138276e-01 -9.20168385e-02  7.02940702e-01  2.15992332e-01\n"," -1.49944946e-01  2.45506316e-01  2.37390399e-01  3.86638530e-02\n","  5.79819977e-01 -1.64111838e-01  4.64468002e-01 -2.78582484e-01\n","  1.39823630e-01 -1.31798223e-01 -9.12342295e-02 -1.64646327e-01\n"," -3.00891727e-01 -1.76722109e-01 -1.28245801e-01  9.90217775e-02\n"," -1.21853769e-01 -3.61223876e-01 -1.24220185e-01  5.26390038e-02\n"," -2.25491658e-01  4.83166501e-02  2.46464148e-01 -2.23851413e-01\n"," -2.36823708e-01  3.59484166e-01 -2.42076695e-01  5.23768008e-01\n","  4.34581488e-01  6.15459085e-02 -4.78574008e-01  8.68931562e-02]\n"]}]}]}